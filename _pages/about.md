---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

News
======

* [Another Mamba-Transformer Hybrid LLM](https://wonmin-byeon.github.io/publication/2024-hymba) is on arxiv! Check the [blog post](https://developer.nvidia.com/blog/hymba-hybrid-head-architecture-boosts-small-language-model-performance/).

* [1 paper](https://wonmin-byeon.github.io/publication/2024-mtvg) accepted to ECCV 2024.

* We've released a new [8B Mamba-based Hybrid LLM](https://wonmin-byeon.github.io/publication/2024-hybrid)! The checkpoints as well as the code are also released as part of [NVIDIA's Megatron-LM project](https://github.com/NVIDIA/megatron-LM/tree/ssm/examples/mamba).

* [1 paper](https://wonmin-byeon.github.io/publication/2024-regiongpt) accepted to CVPR 2024.

* [1 paper](https://wonmin-byeon.github.io/publication/2023-convssm) accepted to NeurIPS 2023.

<!-- * [1 paper](https://wonmin-byeon.github.io/publication/2023-PowerofSound) accepted to ICCV 2023. [2 papers](https://wonmin-byeon.github.io/publications/) accepted to CVPR 2023. -->

<!-- * Our paper ["Physics Informed RNN-DCT Networks for Time-Dependent Partial Differential Equations"](https://wonmin-byeon.github.io/publication/2022-rnndct) is now in [NVIDIA Modulus](https://developer.nvidia.com/modulus)! Check out [technical blog](https://developer.nvidia.com/blog/develop-physics-informed-machine-learning-models-with-graph-neural-networks/) and [Modulus repo](https://github.com/NVIDIA/modulus/tree/main/modulus/models/rnn). -->

Research Interests
======
* Recurrent Neural Network (RNN), State-Space Models (SSM), Linear RNNs

* Sequence Learning, Spatio-Temporal Learning

* Predictive Learning, Few-shot Learning, Lifelong Learning

Selected Projects
======
* X Dong, Y Fu, S Diao, <b>W Byeon</b>, Z Chen, A S Mahabaleshwarkar, S Liu, M Keirsbilck, M Chen, Y Suhara, Y Lin, J Kautz, P Molchanov, ["Hymba: A Hybrid-head Architecture for Small Language Models"](https://wonmin-byeon.github.io/publication/2024-hymba), arXiv, 2024

* R Waleffe, <b>W Byeon</b>, D Riach, B Norick, V Korthikanti, T Dao, A Gu, A Hatamizadeh, S Singh, D Narayanan, G Kulshreshtha, V Singh, J Casper, J Kautz, M Shoeybi, B Catanzaro, ["An Empirical Study of Mamba-based Language Models"](https://wonmin-byeon.github.io/publication/2024-hybrid), arXiv, 2024

* J T.H. Smith, S De Mello, J Kautz, S W. Linderman, <b>W Byeon</b>, ["Convolutional State Space Models for Long-Range Spatiotemporal Modeling"](https://wonmin-byeon.github.io/publication/2023-convssm), NeurIPS, 2023

<!-- * J Su, <b>W Byeon</b>, F Huang, ["Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary Framework"](https://wonmin-byeon.github.io/publication/2022-orthoNN), ICML, 2022 -->

<!-- * B Wu*, O Hennigh, J Kautz, S Choudhry, <b>W Byeon*</b>, ["Physics Informed RNN-DCT Networks for Time-Dependent Partial Differential Equations"](https://wonmin-byeon.github.io/publication/2022-rnndct), ICCS 2022 <b> (*) equal contributions </b>
    - Presented at NeurIPS'21 Workshop on ML and the Physical Science
    - Released as part of [NVIDIA Mudulus](https://developer.nvidia.com/modulus)  -->

<!-- * J Su*, <b>W Byeon*</b>, F Huang, J Kautz, A Anandkumar, ["Convolutional Tensor-Train LSTM for Spatio-temporal Learning"](https://wonmin-byeon.github.io/publication/2020-convttlstm), NeurIPS 2020 <b> (*) equal contributions </b> 
    - Presented at ECCV'20 Tutorial on Accelerating Computer Vision with Mixed Precision. 
 -->    
<!-- * <b>W Byeon</b>, Q Wang, R K Srivastava, P Koumoutsakos, ["ContextVP: Fully Context-Aware Video Prediction"](https://wonmin-byeon.github.io/publication/2018-contextvp), ECCV 2018 (oral) -->

<!-- * PR Vlachas, <b>W Byeon</b>, ZY Wan, TP Sapsis, P Koumoutsakos, ["Data-Driven Forecasting of High-Dimensional Chaotic Systems with Long Short-Term Memory Networks"](https://wonmin-byeon.github.io/publication/2018-05-01-chaotic-lstm), Proceedings of the Royal Society A: Mathematical, Physical & Engineering Sciences. 2018 -->
